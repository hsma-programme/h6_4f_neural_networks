{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - TensorFlow neural net\n",
    "\n",
    "TIP - this workbook contains extensive text to explain the code.  To save you having to constantly scroll up and down to refer back, if you're using VSCode you can right click on the notebook's name tab at the top and click one of the split options (e.g. \"Split Right\") to see have a second view of the same notebook that you can scroll independently.\n",
    "\n",
    "In this workbook we build a neural network to predict passenger survival on the Titanic, using the same dataset we used for the Logistic Regression example. The two common frameworks used for neural networks are TensorFlow and PyTorch. Both are excellent frameworks. TensorFlow frequently requires fewer lines of code, but PyTorch is more natively \"Pythonic\" in its syntax. Here we use TensorFlow and Keras which is integrated into TensorFlow and makes it simpler and faster to build TensorFlow models.\n",
    "\n",
    "**You should install and switch to the supplied tf_hsma environment for this exercise.  This environment contains an installation of TensorFlow version 2.16.1 which you will need.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network unit - a neuron or perceptron\n",
    "\n",
    "The building block of a neural network is a neuron, which is essentially the same as the 'perceptron' described by Frank Rosenblatt in 1958.\n",
    "\n",
    "The neuron, or perceptron, takes inputs *X* and weights *W* (each individual input has a weight; a bias weight is also introduced by creating a dummy input with value 1). The neuron sums the *input* multiplied by the *weight* and passes the output to an activation function. The simplest activation function is a step function, whereby if the output is >0 the output of the activation function is 1, otherwise the output is 0.\n",
    "\n",
    "![](./images/perceptron.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Having understood a neuron - which calculates the weighted sum of its inputs and passes it through an activation function, neural networks are easy(ish)!\n",
    "\n",
    "They are 'just' a network of such neurons, where the output of one becomes one of the inputs to the neurons in the next layer.\n",
    "\n",
    "This allows any complexity of function to be mimicked by a neural network (so long as the network includes a non-linear activation function, like ReLU - see below).\n",
    "\n",
    "Note the output layer may be composed of a single neuron, to predict a single value or single probability, or may be multiple neurons, to predict multiple values or multiple probabilities.\n",
    "\n",
    "![](./images/net_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Each neuron calculates the weighted sum of its inputs and passes that sum to an activation function. The two simplest functions are:\n",
    "\n",
    "* *Linear*: The weighted output is passed forward with no change.\n",
    "\n",
    "* *Step*: The output of the activation function is 0 or 1 depending on whether a threshold is reached.\n",
    "\n",
    "Other common activation functions are:\n",
    "\n",
    "* *Sigmoid*: Scales output 0-1 using a logistic function. Note that our simple single perceptron becomes a logistic regression model if we use a sigmoid activation function. The sigmoid function is often used to produce a probability output at the final layer.\n",
    "\n",
    "* *tanh*: Scales output -1 to 1. Commonly used in older neural network models. Not commonly used now.\n",
    "\n",
    "* *ReLU (rectifying linear unit)*: Simply converts all negative values to zero, and leaves positive values unchanged. This very simple method is very common in deep neural networks, and is sufficient to allow networks to model non-linear functions.\n",
    "\n",
    "* *Leaky ReLU* and *Exponential Linear Unit (ELU)*: Common modifications to ReLU that do not have such a hard constraint on negative inputs, and can be useful if we run into the Dying ReLU problem (in which - typically due to high learning rates - our weights are commonly set to negative values, leading to them effectively being switched off (set to 0) under ReLU). Try them out as replacements to ReLU.\n",
    "\n",
    "* *Maxout*: A generalised activation function that can model a complex non-linear activation function. \n",
    "\n",
    "* *SoftMax*: SoftMax is the final layer to use if you wish to normalise probability outputs from a network which has multiple class outputs (e.g. you want the total of your probabilities for \"this is dog\", \"this is a cat\", \"this is a fish\" etc to add up to 1).\n",
    "\n",
    "![](./images/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Loss functions are critical to neural networks as they provide the measure by which the neural network is in error, allowing modification of the network to reduce error.\n",
    "\n",
    "The most common loss functions are:\n",
    "\n",
    "* *Mean Squared Error Loss*: Common loss function for regression (predicting values rather than class).\n",
    "\n",
    "* *Cross Entropy Loss*: Common loss function for classification. *Binary Cross Entropy Loss* is used when the output is a binary classifier (like survive/die in the Titanic model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do neural networks learn? Backpropagation and optimisation\n",
    "\n",
    "*Backpropagation* is the process by which the final loss is distributed back through the network, allowing each weight to be updated in proportion to its contribution to the final error.\n",
    "\n",
    "For more on backpropagation see: https://youtu.be/Ilg3gGewQ5U\n",
    "\n",
    "For deeper maths on backpropagation see: https://youtu.be/tIeHLnjs5U8\n",
    "\n",
    "*Optimisation* is the step-wise process by which weights are updated. The basic underlying method, *gradient descent*, is that weights are adjusted in the direction that improves fit, and that weights are adjust more when the gradient (how much the output changes with each unit change to the weight) is higher.\n",
    "\n",
    "Common optimisers used are:\n",
    "\n",
    "* *Stochastic gradient descent*: Updates gradients based on single samples. Can be inefficient, so can be modified to use gradients based on a small batch (e.g. 8-64) of samples. *Momentum* may also be added to avoid becoming trapped in local minima.\n",
    "\n",
    "* *RMSprop*: A 'classic' benchmark optimiser. Adjusts steps based on a weighted average of all weight gradients.\n",
    "\n",
    "* *Adam*: The most common optimiser used today. Has complex adaptive momentum for speeding up learning.\n",
    "\n",
    "For more on optimisers see: https://youtu.be/mdKjMPmcWjY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network - the practicalities\n",
    "\n",
    "The training process of a neural network consists of three general phases which are repeated across all the data. All of the data is passed through the network multiple times (the number of iterations, which may be as few as 3-5 or may be 1000+) until all of the data has been fed forward and backpropogated - this then represents an \"Epoch\". The three phases of an iteration are :\n",
    "\n",
    "1. Pass training X data to the network and predict y\n",
    "\n",
    "1. Calculate the 'loss' (error) between the predicted and observed (actual) values of y\n",
    "\n",
    "1. Backpropagate the loss and update the weights (the job of the optimiser).\n",
    "\n",
    "The learning is repeated until maximum accuracy is achieved (but keep an eye on accuracy of test data as well as training data as the network may develop significant over-fitting to training data unless steps are taken to offset the potential for over-fitting, such as use of 'drop-out' layers described below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "\n",
    "The most common fully connected architecture design is to have the same number of neurons in each layer, and adjust that number and the number of layers. This makes exploring the size of the neural net relatively easy (if sometimes slow). \n",
    "\n",
    "As a rough guide - the size of the neural net should be increased until it over-fits data (increasing accuracy of training data with reducing accuracy of test data), and then use a form of *regularisation* to reduce the over-fitting (we will go through this process below).\n",
    "\n",
    "Some common architecture designs, which may be mixed in a single larger network, are:\n",
    "\n",
    "* *Fully connected*: The output of each neuron goes to all neurons in the next layer.\n",
    "\n",
    "* *Convolutional*: Common in image analysis. Small 'mini-nets' that look for patterns across the data - like a 'sliding window', but that can look at the whole picture at the same time. May also be used, for example, in time series to look for fingerprints of events anywhere in the time series.\n",
    "\n",
    "* *Recurrent*: Introduce the concept of some (limited) form of memory into the network - at any one time a number of input steps are affecting the network output. Useful, for example, in sound or video analysis.\n",
    "\n",
    "* *Transformers*: Sequence-to-sequence architecture. Convert sequences to sequences (e.g. translation).  Big in Natural Language Processing - we'll cover them in the NLP module.\n",
    "\n",
    "* *Embedding*: Converts a categorical value to a vector of numbers, e.g. word-2-vec converts words to vectors such that similar meaning words are positioned close together.\n",
    "\n",
    "* *Encoding*: Reduce many input features to fewer. This 'compresses' the data. De-coding layers may convert back to the original data.\n",
    "\n",
    "* *Generative*: Rather than regression, or classification, generative networks output some form of synthetic data (such as fake images; see https://www.thispersondoesnotexist.com/).\n",
    "\n",
    "For the kind of classification problem we're looking at here, a Fully Connected Neural Network is the most commonly used architecture now, and typically you keep all layers the same size (the same number of Neurons) apart from your output layer.  This makes it easy to test different sizes of network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "Also see the excellent introductory video (20 minutes) from 3brown1blue: https://youtu.be/aircAruvnKk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first cell, we're going to be a bit naughty, and turn off warnings (such as \"you're using an out-of-date version of this\" etc).  This will make the notebook cleaner and easier to interpret as you learn this, but in real-world work you shouldn't really do this unless you know what you're doing.  But we'll do it here because we do (I think).\n",
    "\n",
    "Don't forget to select the tf_hsma environment when you run the first cell.  If you're prompted that you need to install the ipykernel, click that you want to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the packages we're going to use.  The first three (MatPlotLib, NumPy and Pandas) are the stuff we use in pretty much everything in data science.  From SciKitLearn, we import functions to automatically split our data into training and test data (as we did for the Logistic Regression example) and to min-max normalise our data (remember we said that normalising our data is typical with Neural Networks (\"Neural Networks are Normal\"), and standardising our data - what we did last time - is typical with Logistic Regression).  Remember, when we normalise we'll scale all our feature values so they fall between 0 and 1.\n",
    "\n",
    "Then, we import a load of things we'll need from TensorFlow (and particularly Keras).  TensorFlow is the Neural Network architecture developed by Google, but the interface (API) for TensorFlow is not easy to use.  So instead, we use Keras, which sits on top of TensorFlow, and allows us to interact with TensorFlow in a much more straightforward way.  Don't worry about what each of things that we import are at this stage - we'll see them in use as we move through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TensorFlow sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell downloads the Titanic data that we're going to use.  You don't need to do this if you've already downloaded the data, but if you're unsure, run the cell anyway (it takes seconds!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scale data\n",
    "\n",
    "In neural networks it is common to normalise (scale input data 0-1) rather than use standardise (subtracting mean and dividing by standard deviation) each feature.  As with the Logistic Regression example, we'll set up a function here that we can call whenever we want to do this (the only difference being that in the Logistic Regression example we standardised our data, rather than normalising it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.fit_transform(X_train)\n",
    "    test_sc = sc.fit_transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load up and do a bit of initial prep on our data, much as we did before for the Logistic Regression.  We're going to load our data (which is stored in a .csv file) into a Pandas DataFrame.  We'll convert all the data into floating point numbers so everything is consistent.  We'll drop the Passenger ID column, as that isn't part of the original data, and we don't want the machine to learn anything from this.\n",
    "\n",
    "Then we define our input (X) and output (y) data.  Remember we're trying to predict y from X.  X is all of our columns (features) except for the \"Survived\" column (which is our label - the thing we're trying to predict).  The `axis=1` argument tells Pandas we're referring to columns when we tell it to drop stuff.\n",
    "\n",
    "We also set up NumPy versions of our X and y data - this is a necessary step if we were going to do k-fold splits (remember we talked about those in the last session - it's where we split up our data into training and test sets in multiple different ways to try to avoid biasing the data) as it requires the data to be in NumPy arrays, not Pandas DataFrames.  We're not actually going to use k-fold splits in this workbook, but we'll still go through the step of getting the data into the right format for when we do.  Because, in real world applications, you *should* use k-fold splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net\n",
    "\n",
    "We're going to put construction of the neural net into a separate function, which is what we've written in this next cell.  The function that we've written will build a network of any size, but the one we set up here will be relatively simple.  As you read each bit in this description, you should scroll down to look at the corresponding code in the next cell.\n",
    "\n",
    "Here we use the `sequential` method to set up a TensorFlow neural network. This simpler method assumes each layer of the Neural Network occurs in sequence (one layer of neurons feeds into the next layer of neurons etc).  There are more complex architectures, but the simple sequential architecture is common.  Though simpler, it lacks some flexibility.  But don't worry about that for the moment.\n",
    "\n",
    "The inputs into the function are as follows :\n",
    "\n",
    "- The number of features in our input data (the columns of data from which we hope to make a prediction)\n",
    "- The number of hidden layers - these are the layers between our input layer and our output layer.  Determining how many works best for your problem is a bit of an art-form, but you'll probably find something between 1 and 7 hidden layers works well for classification problems.  We set a default of 3, so if we don't specify a number when calling the function, it'll default to 3 hidden layers. (The assignment operator `=` denotes we are specifying a default value)\n",
    "- The number of neurons in each hidden layer.  We default to 128 if no number is specified when the function is called.\n",
    "- The dropout rate (the proportion of neurons that will be randomly \"switched off\" in each epoch to try to prevent overfitting.  We default to no dropout if no value is specified when the function is called.\n",
    "- A learning rate, that will be used by the optimiser to determine how much it should change each time in response to the estimated error.  In other words, how sensitive will it be.  As with most things in a Neural Network, you will spend a lot of time playing around with these parameters to see if it improves things, but a good default is 0.003 (although for some non-Adam optimisers, that can be considered quite high, so you might want to try much lower learning rates to see if they help).  We use 0.003 as the default if no value is specified when the function is called.\n",
    "\n",
    "In the function, we first clear the session.  There used to be issues with TensorFlow keeping old models hanging around in memory - whilst it probably isn't an issue now, we still do it just to be on the safe side.\n",
    "\n",
    "Then we create a new Sequential Neural Network.\n",
    "\n",
    "Next, we use a loop to set up the hidden layers of our network (the input layer is added automatically and we don't need to explicitly define it).  For each pass of the *for loop* (you'll see we go around the loop the number of times we have passed in as being the number of hidden layers we want) we :\n",
    "\n",
    "- Add a fully-connected (dense) layer (one in which all of the neurons in the layer are connected to all of the neurons in the next layer), using the ReLU activation function for its neurons (you could of course change this to another activation function of your choosing)\n",
    "- Add a dropout layer, to enable dropout\n",
    "\n",
    "So, we end up with a set of fully-connected layers, each with a dropout layer after it.\n",
    "\n",
    "Once we've added all our hidden layers we then add our final output layer.  Here, we add a densely connected output layer with 1 neuron (which will calculate the output - the prediction) using the Sigmoid activation function (because we want our output to be a kind of \"likelihood\" of it being a classified as a '1' - whatever that represents in our model.  By default, anything over 0.5 will be classified as 1, whilst everything else will be classified as 0).  Note of caution - do not interpret this number as being a probability that something belongs to a certain class, it's subtly different (so for example, you shouldn't see that a passenger has an output value of 0.6, and interpret that that they had a 60% probability of survival.  That's NOT correct.  Rather, the model has classified them as likely a survivor with a small amount of confidence (it's only just past the threshold of 0.5)).\n",
    "\n",
    "Next we add our optimiser engine.  Remember - if in doubt, use Adam :)\n",
    "\n",
    "Then we \"compile\" the Neural Network (this is just a fancy way of saying we'll tell TensorFlow to build what we've set up above).  When we tell it to compile, we tell it the loss function we want to use (here - binary crossentropy, because it's a classification problem with two possible outputs - for the Titanic problem, this is \"survived\" or \"died\").  We also specify the metrics that we want TensorFlow to monitor as the model is being \"fitted\" (learning) - here we tell it to monitor accuracy.\n",
    "\n",
    "Finally, we get the function to return the network so we can use it (remember - the purpose of this function is to build a network to our specifications when we call it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, \n",
    "             hidden_layers=3, \n",
    "             hidden_layer_neurones=128, \n",
    "             dropout=0.0, \n",
    "             learning_rate=0.003):\n",
    "    \n",
    "    \"\"\"Make TensorFlow neural net\"\"\"\n",
    "    \n",
    "    # Clear Tensorflow \n",
    "    K.clear_session()\n",
    "    \n",
    "    # Set up neural net\n",
    "    net = Sequential()\n",
    "    \n",
    "    # Add hidden hidden_layers using a loop\n",
    "    for i in range(hidden_layers):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            hidden_layer_neurones, \n",
    "            input_dim=number_features,\n",
    "            activation='relu'))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(dropout))\n",
    "    \n",
    "    # Add final sigmoid activation output\n",
    "    net.add(Dense(1, activation='sigmoid'))    \n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    net.compile(loss='binary_crossentropy', \n",
    "                optimizer=opt, \n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary of the model structure\n",
    "\n",
    "Here we will create an arbitrary model (that we won't use) with 10 input features, just to show the function we wrote above being used and so you can see how you can use the summary() function of a model to see an overview of the structure of it.\n",
    "\n",
    "We can see what the layers are in order.  Remember we have five main layers in total (input, 3 x hidden, output) but you won't see the input layer here.  When you run the below cell, you should see three hidden layers (each with a dropout layer immediately after) with 128 neurons in each layer followed by a final output layer with just one neuron.  You'll also see that there are over 34,500 parameters (weights) that it needs to optimise, just in a very simple network like this on a very small dataset with 10 features.  Now you can see why they're so complicated (and magical!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_net(10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Scale data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before with the Logistic Regression, we split our data into training and test sets.  We've got 25% carved off for our test set.  But what is this `random_state=42` thing?  So, pulling back the curtain now, but the random numbers we tend to generate in our computers are not *strictly* random.  They are *pseudo-random* - they use complex algorithms to generate numbers that appear random (and which are good enough for the vast majority of things you will ever do).  Because they are pseudo-random, this means that we can fix the random number generator to use a pre-defined *seed* - a number that feeds into the algorithm which will ensure we always get the same random numbers being generated.  This can be useful if we're 1) teaching, and you want everyone to get the same thing, or 2) validating our outputs whilst we build our model.  Since we're doing both of those things here, we use a fixed seed.\n",
    "\n",
    "But why the number 42?  Those of you who have read, watched and / or listened to The Hitchiker's Guide to the Galaxy will know why.  Those that haven't, go off and read, watch or listen to it and then you'll get the \"joke\" (Computer Scientists love doing stuff like this..)\n",
    "\n",
    "Once we've established our training and testing data, we scale the data by normalising it, using the function we wrote earlier (which uses min-max normalisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_np, y_np, test_size = 0.25, random_state=42)\n",
    "\n",
    "# Scale X data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just have a look at the scaled data for the first two records (passengers) in our input data.  We *should* see that all of the feature values have scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this with the unscaled data for the same two passengers to see the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to write a little function that will report the accuracy of the model on the training set and the test set.  This will help us assess how well our model is performing.  We pass into the function the model, the (normalised) input data for both the training and test sets, and the output data for both the training and test sets.\n",
    "\n",
    "The function uses the `predict` function of the model to grab out the probability predictions based on the input data for the training set.  We specify that a classification of 1 (in the case of Titanic, this means \"survived\") should be made if the probability predicted is greater than 0.5.  Then we \"flatten\" the data to get it in the right shape (because it comes out as a complex shape - a tensor.  Don't worry about this.  Just imagine a blob of data, and we squish it so we can read it).  Then we use `y_pred_train == y_train` to return boolean `True` values for each time where the prediction (survived or died) matched the real answer, and take the average of those matches (that effectively gives us accuracy - what proportion of times did prediction match real answer).  (Python interprets Trues and Falses as 1s and 0s, in case you're wondering how that works!).\n",
    "\n",
    "Then we do the same as above but for the test set.\n",
    "\n",
    "Finally we print the accuracy on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n",
    "    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"    \n",
    "    \n",
    "    ### Get accuracy of fit to training data\n",
    "    probability = model.predict(X_train_sc)\n",
    "    y_pred_train = probability >= 0.5\n",
    "    y_pred_train = y_pred_train.flatten()\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    \n",
    "    ### Get accuracy of fit to test data\n",
    "    probability = model.predict(X_test_sc)\n",
    "    y_pred_test = probability >= 0.5\n",
    "    y_pred_test = y_pred_test.flatten()\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "    # Show acuracy\n",
    "    print (f'Training accuracy {accuracy_train:0.3f}')\n",
    "    print (f'Test accuracy {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also write a little function to plot the accuracy on the training set and the test set over time.  Keras keeps a \"history\" (which is a dictionary) of the learning which allows us to do this easily.  It's quite useful to plot the performance over time, as it allows us to look for indications as to when the model is becoming overfitted etc.\n",
    "\n",
    "In our function, we'll grab out the values from the passed in history dictionary, and then plot them using standard matplotlib plotting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict):\n",
    "    acc_values = history_dict['accuracy']\n",
    "    val_acc_values = history_dict['val_accuracy']\n",
    "    epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "\n",
    "    ax.plot(epochs, acc_values, color='blue', label='Training acc')\n",
    "    ax.plot(epochs, val_acc_values, color='red', label='Test accuracy')\n",
    "    ax.set_title('Training and validation accuracy')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now defined everything that will allow us to build the model.  So we'll now define the model we want and train it!\n",
    "\n",
    "To work out how many features we need (which we then need to pass into the `make_net` function we defined earlier), we can simply look at the number of columns in our X (input) data (where we've removed the 'label' (output) column).  We can grab this from the standardised training data, by looking at index 1 of the shape tuple (index 0 would be rows (passengers in the Titanic data), and index 1 would be columns).  We can see this if we run the code `X_train_sc.shape`.  Try it yourself (just insert a code cell below this markdown cell)!  You should see there are 668 rows, and 24 columns.  Therefore, we've got 668 passengers and 24 features.\n",
    "\n",
    "Next we call our `make_net` function, passing in the number of features we calculated above.  This will create our Neural Network.  As we've passed in nothing else, we'll have defaults for the rest of the network - 3 hidden layers, 128 neurons per layer, a learning rate of 0.003 and no dropout (although, we will still have dropput layers, they just won't do anything).\n",
    "\n",
    "Then, we fit (train) the model.  To do that, we call the `fit` method of the model, and pass it :\n",
    "\n",
    "- the standardised training data\n",
    "- the output (label) data\n",
    "- the number of epochs (training generations - full passes of all of the data through the network).  Initially, we want enough epochs that we see overfitting start to happen (the training accuracy starts to plateau) because then we know we've trained \"enough\" (albeit a bit too much) and can then look to reduce it back a bit\n",
    "- the batch size (how much data we shunt through the network at once.  Yann LeCun (French Computer Scientist) advises \"Friends shouldn't let friends use batch sizes of more than 32\".  But we will here... :))\n",
    "- the data we want to use as our \"validation data\" (which we use to fine tune the parameters of the model).  Keras will check performance on this validation data.  Here we just use our test set, but you should really have a separate \"validation set\" that you'd use whilst tuning the model.\n",
    "- whether we want to see all the things it's doing as it's learning.  If we set `verbose` to 0, all of this will be hidden (keeping things tidier), but as we're experimenting with our model, it's a good idea to set `verbose` to 1 so we can monitor what it's doing.\n",
    "\n",
    "You'll also see that we not only call `model.fit` but we store the output of that function in a variable called `history`.  This allows us to access all the useful information that keras was keeping track of whilst the model was training.  We'll use that later.\n",
    "\n",
    "Note - when you run the cell below, the model will be built and then start training.  How long this takes will depend on your computer specs, including whether you have a CUDA-enabled GPU (if you're running locally) or your priority in the queue for cloud computing (if you're running this on CoLab).\n",
    "\n",
    "Dan has a very fast computer with a high performance CUDA-enabled GPU, and the below (with 250 epochs) takes about 6 seconds on the GPU and about 11 seconds on the CPU.  It might take a little while longer on yours - don't worry, as long as you can see it moving through the epochs.\n",
    "\n",
    "For each epoch, you'll see various information, including the epoch number, the loss (error) that's been calculated in that epoch (for both the training and validation data), and the accuracy (for both the training and validation data).  You should see loss gradually reduce, and accuracy gradually increase.  But you'll likely see that training accuracy tends to keep getting better (before it reaches a plateau) and validation accuracy gets better but then starts to drop a bit.  That's a sign of overfitting (our model's become increasingly brilliant for the training data, but starting to get increasingly rubbish at being more generally useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "\n",
    "### Train model (and store training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate and print the final accuracy scores for both the training and test (validation) data.  Remember, we'll call the function we wrote to do this earlier.  You should see training accuracy is much better than test accuracy.  We've overfitted.  Don't worry - we'll try and improve that in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training history\n",
    "\n",
    "`history` is a dictionary containing data collected during training.  Remember - we stored it when we called the `model.fit()` method.  Let's take a look at the keys in this dictionary (these are the metrics monitored during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above that we have four keys in our history dictionary - loss, accuracy, validation loss and validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our history data using the plotting function we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see from the plot above that the training accuracy gets better and better before reaching a plateau, but for the test data the accuracy initially improves, but then reduces a bit and plateaus at poorer performance.  As we thought, we've overfitted.  So let's look at how we can now try to reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving fit by avoiding or reducing-over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, we discussed a number of strategies we can take to try to reduce overfitting.  Let's look at each in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Reduce complexity of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple initial strategy is to reduce the complexity of the model, so that the \"dividing line\" it learns becomes less complex (and less likely to be an overfit).\n",
    "\n",
    "Here, we create a new model where we reduce the number of hidden layers to 1 (from the default we used of 3), and we reduce the number of neurons on each hidden layer to 32 (from the default we used of 128).\n",
    "\n",
    "Then we fit (train) this new model, exactly as we did before.  We'll set `verbose` to 0 though, so we don't see everything as it trains (if you'd rather see it, just change `verbose` to 1 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features,\n",
    "                hidden_layers=1,\n",
    "                hidden_layer_neurones=32)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate, print and plot accuracy as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the simplification of the model above has improved things a bit (though it may not, there's randomness at play here, and your network may have learned differently) - training accuracy has reduced, but test accuracy (our measure of how generally useful our model will be beyond the training set) has improved - a little bit.  But there's still a bit of a gap between them - we're still overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Reduce training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, let's do one change at a time, so we'll go back to our original model before trying our next strategy.\n",
    "\n",
    "Another approach we can use is simply to stop training for so long.  We can see from our earlier plots that things improve in the test set initially but then reduces.  So, by not training for so long, we can stop training before it significantly overfits.\n",
    "\n",
    "Here, we'll run the model exactly as we did the first time, except we'll only run it for 25 epochs, rather than 250 - just 10% of the original training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that reducing the training time has also led to an improvement in test accuracy, much as simplifying the model did, although you might not.  You might find that this measure is slightly more effective than the simplifying measure.  You should also see from the plot that the test set accuracy tends to plateau, and it doesn't get to the bit where it starts dropping significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Add dropout\n",
    "\n",
    "Using dropout, in each training epoch a random selection of weights are \"switched off\" (the selection changes from epoch to epoch).  It does this by using the Dropout layers after each hidden layer (remember when we added those earlier?), and randomly switching some of the incoming weights to 0.  When predicting (after fitting) all weights are used.  Dropout ensures that, during training, the model can't rely too much on any set of weights (because they'll occasionally be turned off), and looks to explore them more globally.\n",
    "\n",
    "This is probably the most common method for reducing overfitting.  Dropout values of 0.2 to 0.5 are common.\n",
    "\n",
    "Here, we'll use a dropout value of 0.5.  So 50% of the weights coming out of each hidden layer will be set to 0 in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features,\n",
    "                dropout=0.5)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show acuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we should see that Dropout has improved performance on the test set over the base case (although it might not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Combination of the above and with automatic early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just doing one of these things above, we tend to combine these measures.  We'll also use a Keras callback called EarlyStopping to automate the measure where we try to stop the training sooner.  A callback is simply a function that Keras can use to perform various actions continually throughout the training.\n",
    "\n",
    "EarlyStopping will automatically stop the training when it appears the validation accuracy isn't getting any better.  It allows us to specify a `patience` level, which is the number of epochs we are prepared to wait (to give it a chance to improve) before EarlyStopping cuts things off.  We can also optionally specify the minimum level we want our metric(s) (e.g. accuracy) to improve between epochs to count as an \"improvement\" - this allows us to say that we don't consider a very small improvement as significant enough.  You'll see examples of this later in the course, but here we'll just specify `patience`, and we'll allow any improvement to count as improvement.\n",
    "\n",
    "Here, we specify a `patience` of 25 epochs - this means that we are prepared to wait 25 epochs to see if we can get a better accuracy score on the validation set.  By setting `restore_best_weights=True` we tell it that, once it stops (if it didn't manage to improve things in 25 epochs), then it should roll back the network to how it was when it reached its peak performance.\n",
    "\n",
    "So, here we set up our EarlyStopping callback.  Then we define a simpler network with 1 hidden layer and 64 neurons per layer, have a 50% dropout rate, and run for 250 epochs **but** add in the EarlyStopping callback so that Keras will stop the training when things stop improving in the validation set, and revert back to the best version it's seen.\n",
    "\n",
    "In the below, you'll see we've also added another callback called ModelCheckpoint.  This callback just automatically saves the model at its best point so we can easily retrieve it.  In combination with EarlyStopping, this means we have a model that won't keep going beyond when it should, and it'll save the best version for later use.\n",
    "\n",
    "Note that as well as creating and defining the callbacks, you also need to ensure you add them into the list of inputs you pass in when you call `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback\n",
    "# Stop when no validation improvement for 25 epochs\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=25, restore_best_weights=True)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(\n",
    "    number_features,\n",
    "    hidden_layers=1,\n",
    "    hidden_layer_neurones=64,\n",
    "    dropout=0.5)\n",
    "\n",
    "### Train model (and stote training info in history)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show accuracy\n",
    "calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see from the above, where we've combined the three anti-overfitting measures, that we get quite a decent improvement in test accuracy and a closing of the gap between training and test accuracy.  This indicates that our model is far less overfitted than it was originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reloading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll look at how we can save our models so we can come back to them another time, and we don't have to retrain them each time.  For a small model like this, it's not hugely inconvenient, but if we had a large model (that could take hours or even days to run) we don't want to have to retrain it every time we want to use it!\n",
    "\n",
    "Here, we can use the `save()` function of the model to easily save a model.  We just pass in a filename - we use the new .keras file extension.  The model will be saved in the present working directory for the code.\n",
    "\n",
    "You can also see in the cell below how to load a model back in, and then use it again.  You can verify this if you run the two cells below, which will save the model, then load it back up, and recalculate its accuracy - you should see that the reported training and test accuracies are the same as you had above (because that's the model we saved and then loaded back up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('titanic_tf_model.keras')\n",
    "\n",
    "# Load and use saved model - we need to first set up a model\n",
    "restored_model = keras.models.load_model('titanic_tf_model.keras')\n",
    " \n",
    "# Predict classes as normal\n",
    "predicted_proba = restored_model.predict(X_test_sc)\n",
    "\n",
    "# Show examples of predicted probability\n",
    "print(predicted_proba[0:5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(restored_model, X_train_sc, X_test_sc, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
